{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def rolling_forecast_AR(df, window_size_train=720, window_size_test=24, step_size=1):\n",
    "    results = []\n",
    "    total_length = len(df)\n",
    "    \n",
    "    for i in range(0, total_length - window_size_train + 1, step_size):\n",
    "        train_ar = df.iloc[i:i+window_size_train]\n",
    "        \n",
    "        remaining_length = total_length - i - window_size_train\n",
    "        test_ar = df.iloc[i+window_size_train:i+window_size_train+min(window_size_test, remaining_length)]\n",
    "        \n",
    "        model = AR(train_ar)\n",
    "        model_fit = model.fit()\n",
    "        window = model_fit.k_ar\n",
    "        coef = model_fit.params\n",
    "        history = train_ar[-window:]\n",
    "        history = [history[i] for i in range(len(history))]\n",
    "        predictions = []\n",
    "\n",
    "        for t in range(len(test_ar)):\n",
    "            length = len(history)\n",
    "            lag = [history[i] for i in range(length-window, length)]\n",
    "            yhat = coef[0]\n",
    "            for d in range(window):\n",
    "                yhat += coef[d+1] * lag[window-d-1]\n",
    "            obs = test_ar[t]\n",
    "            predictions.append(yhat)\n",
    "            history.append(obs)\n",
    "        \n",
    "        # Calculate standard errors for predictions\n",
    "        stderr = np.sqrt(model_fit.sigma2)\n",
    "        conf_int = pd.DataFrame(index=test_ar.index, columns=['Lower CI', 'Upper CI'])\n",
    "        conf_int['Lower CI'] = np.array(predictions) - 1.96 * stderr\n",
    "        conf_int['Upper CI'] = np.array(predictions) + 1.96 * stderr\n",
    "        \n",
    "        raw_diff = np.mean(test_ar - predictions)\n",
    "        mse = mean_squared_error(test_ar, predictions)\n",
    "        mae = mean_absolute_error(test_ar, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': 'AR',\n",
    "            'Start Timestamp': test_ar.index[0],\n",
    "            'End Timestamp': test_ar.index[-1],\n",
    "            'Raw Difference': raw_diff,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "rolling_metrics = rolling_forecast_AR(df_resampled['G7 - NormalizedPnlRate'], window_size_train=720, window_size_test=24, step_size=720)\n",
    "\n",
    "# Plotting example for the last iteration\n",
    "train_ar = df_resampled['G7 - NormalizedPnlRate'].iloc[-720-24:-24] \n",
    "test_ar = df_resampled['G7 - NormalizedPnlRate'].iloc[-24:]\n",
    "model = AR(train_ar)\n",
    "model_fit = model.fit()\n",
    "window = model_fit.k_ar\n",
    "coef = model_fit.params\n",
    "history = train_ar[-window:]\n",
    "history = [history[i] for i in range(len(history))]\n",
    "predictions = []\n",
    "\n",
    "for t in range(len(test_ar)):\n",
    "    length = len(history)\n",
    "    lag = [history[i] for i in range(length-window, length)]\n",
    "    yhat = coef[0]\n",
    "    for d in range(window):\n",
    "        yhat += coef[d+1] * lag[window-d-1]\n",
    "    obs = test_ar[t]\n",
    "    predictions.append(yhat)\n",
    "    history.append(obs)\n",
    "\n",
    "stderr = np.sqrt(model_fit.sigma2)\n",
    "conf_int = pd.DataFrame(index=test_ar.index, columns=['Lower CI', 'Upper CI'])\n",
    "conf_int['Lower CI'] = np.array(predictions) - 1.96 * stderr\n",
    "conf_int['Upper CI'] = np.array(predictions) + 1.96 * stderr\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_resampled.index[-600:], df_resampled['G7 - NormalizedPnlRate'].tail(600), color='green', label='Close price')\n",
    "plt.plot(test_ar.index, test_ar, color='red', label='Test close price')\n",
    "plt.plot(test_ar.index, predictions, color='blue', label='Predicted close price')\n",
    "plt.fill_between(test_ar.index, conf_int['Lower CI'], conf_int['Upper CI'], color='gray', alpha=0.3, label='95% CI')\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_resampled.index[-100:], df_resampled['G7 - NormalizedPnlRate'].tail(100), color='green', label='Close price')\n",
    "plt.plot(test_ar.index, test_ar, color='red', label='Test close price')\n",
    "plt.plot(test_ar.index, predictions, color='blue', label='Predicted close price')\n",
    "plt.fill_between(test_ar.index, conf_int['Lower CI'], conf_int['Upper CI'], color='gray', alpha=0.3, label='95% CI')\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Raw Difference:', np.mean(test_ar - predictions))\n",
    "print('MSE:', mean_squared_error(test_ar, predictions))\n",
    "print('MAE:', mean_absolute_error(test_ar, predictions))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(test_ar, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to hourly data (assuming your data is at a higher frequency)\n",
    "df_resampled = df.resample('H').mean()\n",
    "\n",
    "# Preprocess to handle weekends\n",
    "def merge_weekends(df):\n",
    "    indices_to_keep = []\n",
    "    for i in range(len(df.index)):\n",
    "        if df.index[i].dayofweek == 5:  # Friday\n",
    "            if i + 2 < len(df.index) and df.index[i + 2].dayofweek == 0:  # Monday\n",
    "                indices_to_keep.append(i)\n",
    "                indices_to_keep.append(i + 2)\n",
    "            elif i + 1 < len(df.index) and df.index[i + 1].dayofweek == 0:  # Only keep Friday if Monday follows immediately\n",
    "                indices_to_keep.append(i)\n",
    "                indices_to_keep.append(i + 1)\n",
    "        elif df.index[i].dayofweek < 5 or df.index[i].dayofweek == 6:  # Keep all other days\n",
    "            indices_to_keep.append(i)\n",
    "    return df.iloc[indices_to_keep]\n",
    "\n",
    "df_resampled = merge_weekends(df_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to hourly data (assuming your data is at a higher frequency)\n",
    "df_resampled = df.resample('H').mean()\n",
    "\n",
    "# Preprocess to handle weekends\n",
    "def merge_weekends(df):\n",
    "    indices_to_keep = []\n",
    "    for i in range(len(df.index)):\n",
    "        if df.index[i].dayofweek == 5:  # Friday\n",
    "            if i + 2 < len(df.index) and df.index[i + 2].dayofweek == 0:  # Monday\n",
    "                indices_to_keep.append(i)\n",
    "                indices_to_keep.append(i + 2)\n",
    "            elif i + 1 < len(df.index) and df.index[i + 1].dayofweek == 0:  # Only keep Friday if Monday follows immediately\n",
    "                indices_to_keep.append(i)\n",
    "                indices_to_keep.append(i + 1)\n",
    "        elif df.index[i].dayofweek < 5 or df.index[i].dayofweek == 6:  # Keep all other days\n",
    "            indices_to_keep.append(i)\n",
    "    return df.iloc[indices_to_keep]\n",
    "\n",
    "df_resampled = merge_weekends(df_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_smooth_data(csv_path, output_path, span=24):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')\n",
    "\n",
    "    # Calculate the EWMA smoothened data\n",
    "    df['EWMA_NormalizedPnL'] = df['NormalizedPnL'].ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    # Resample the index to hourly intervals\n",
    "    df_resampled = df.resample('H').mean()\n",
    "\n",
    "    # Interpolate NaN values\n",
    "    df_resampled = df_resampled.interpolate()\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV\n",
    "    df_resampled.to_csv(output_path)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "# Example usage\n",
    "df_processed = process_and_smooth_data('your_data.csv', 'resampled_and_smoothened_data.csv', span=24)\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess(csv_path):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')\n",
    "\n",
    "    # Resample the data to hourly intervals and interpolate missing values\n",
    "    df_resampled = df.resample('H').mean().interpolate()\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_resampled = load_and_preprocess('your_data.csv')\n",
    "print(df_resampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df['lag1'] = df['NormalizedPnL'].shift(1)\n",
    "    df['lag2'] = df['NormalizedPnL'].shift(2)\n",
    "    df['lag3'] = df['NormalizedPnL'].shift(3)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_features = create_features(df_resampled)\n",
    "print(df_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_xgboost_model(df):\n",
    "    # Create the feature and target variables\n",
    "    X = df[['hour', 'dayofweek', 'month', 'lag1', 'lag2', 'lag3']]\n",
    "    y = df['NormalizedPnL']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "    model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_test.index, y_test, label='Actual')\n",
    "    plt.plot(y_test.index, y_pred, label='Forecasted')\n",
    "    plt.legend()\n",
    "    plt.title('Actual vs Forecasted NormalizedPnL')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Root Mean Squared Error: {rmse}')\n",
    "    \n",
    "    return model, y_test, y_pred\n",
    "\n",
    "model, y_test, y_pred = train_xgboost_model(df_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "csv_path = 'your_data.csv'\n",
    "df_resampled = load_and_preprocess(csv_path)\n",
    "df_features = create_features(df_resampled)\n",
    "model, y_test, y_pred = train_xgboost_model(df_features)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

def load_and_preprocess(csv_path):
    # Load the data
    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')

    # Resample the data to hourly intervals and interpolate missing values
    df_resampled = df.resample('H').mean().interpolate()

    return df_resampled

def rolling_arma_forecast(df, target_col, window_size_train=720, window_size_test=24, step_size=1):
    results = []
    total_length = len(df)
    
    for i in range(0, total_length - window_size_train - window_size_test + 1, step_size):
        # Define train and test sets for this window
        train = df[target_col].iloc[i:i+window_size_train]
        test = df[target_col].iloc[i+window_size_train:i+window_size_train+window_size_test]

        # Auto-select the best ARMA order based on AIC
        best_aic = np.inf
        best_order = None
        best_model = None

        for p in range(1, 5):
            for q in range(1, 5):
                try:
                    model = ARIMA(train, order=(p, 0, q))
                    model_fit = model.fit()
                    aic = model_fit.aic
                    if aic < best_aic:
                        best_aic = aic
                        best_order = (p, q)
                        best_model = model_fit
                except:
                    continue
        
        # Forecast for the test period
        forecast = best_model.forecast(steps=window_size_test)
        actual_values = test.values
        
        # Calculate metrics
        raw_diff = forecast - actual_values
        abs_diff = np.abs(raw_diff)
        rmse = np.sqrt(np.mean(abs_diff ** 2))
        
        # Store overall average metrics for the test window
        results.append({
            'Start Timestamp': test.index[0],  # Timestamp of the first hour in the test window
            'End Timestamp': test.index[-1],  # Timestamp of the last hour in the test window
            'Best Order': best_order,
            'Raw Difference': np.mean(raw_diff),
            'Absolute Difference': np.mean(abs_diff),
            'RMSE': rmse
        })
    
    return pd.DataFrame(results)

def plot_forecast(df, results, target_col):
    plt.figure(figsize=(14, 7))
    plt.plot(df.index, df[target_col], label='Actual', color='green')
    
    for idx, row in results.iterrows():
        plt.plot(pd.date_range(row['Start Timestamp'], row['End Timestamp'], freq='H'), [row['Raw Difference']] * len(pd.date_range(row['Start Timestamp'], row['End Timestamp'], freq='H')), label='Forecast', linestyle='--')
    
    plt.legend()
    plt.title('Actual vs Forecasted NormalizedPnL')
    plt.show()

# Example usage
csv_path = 'your_data.csv'
df_resampled = load_and_preprocess(csv_path)
rolling_metrics = rolling_arma_forecast(df_resampled, 'NormalizedPnL', window_size_train=720, window_size_test=24, step_size=720)
print(rolling_metrics)
plot_forecast(df_resampled, rolling_metrics, 'NormalizedPnL')


import pandas as pd

def process_csv_and_calculate_stats(csv_path):
    # Load the data
    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')

    # Add a date column
    df['Date'] = df.index.date

    # Group by date
    grouped = df.groupby('Date')

    # Calculate proportions of each state
    state_proportions = grouped['State'].value_counts(normalize=True).unstack(fill_value=0)

    # Calculate mean and std for each group
    mean_std = grouped['Value'].agg(['mean', 'std'])

    # Merge the proportions with the mean and std
    result = state_proportions.join(mean_std)

    return result

# Example usage
csv_path = 'your_data.csv'
result_df = process_csv_and_calculate_stats(csv_path)
print(result_df)

import pandas as pd

def process_csv_and_calculate_stats(csv_path):
    # Load the data
    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')

    # Add a date and day of the week column
    df['Date'] = df.index.date
    df['DayOfWeek'] = df.index.dayofweek

    # Adjust the date for Sunday to merge with Monday
    df['AdjustedDate'] = df['Date']
    df.loc[df['DayOfWeek'] == 6, 'AdjustedDate'] = df['Date'] + pd.Timedelta(days=1)

    # Group by the adjusted date
    grouped = df.groupby('AdjustedDate')

    # Calculate proportions of each state
    state_proportions = grouped['State'].value_counts(normalize=True).unstack(fill_value=0)

    # Calculate mean and std for each group
    mean_std = grouped['Value'].agg(['mean', 'std'])

    # Determine the state with the highest proportion for each adjusted date
    best_state = state_proportions.idxmax(axis=1)
    best_state.name = 'Best State'

    # Get the best states in order from best to worst
    best_states_sorted = state_proportions.apply(lambda row: row.sort_values(ascending=False).index.tolist(), axis=1)
    best_states_sorted.name = 'Best States Sorted'

    # Merge the proportions, mean, std, and best states sorted
    result = state_proportions.join(mean_std).join(best_state).join(best_states_sorted)

    return result, best_states_sorted

# Example usage
csv_path = 'your_data.csv'
result_df, best_states_sorted = process_csv_and_calculate_stats(csv_path)
print(result_df)
print(best_states_sorted)

import pandas as pd

def process_and_calculate_diff(csv_path):
    # Load the data
    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')

    # Add a date and day of the week column
    df['Date'] = df.index.date
    df['DayOfWeek'] = df.index.dayofweek

    # Adjust the date for Sunday to merge with Monday
    df['AdjustedDate'] = df['Date']
    df.loc[df['DayOfWeek'] == 6, 'AdjustedDate'] = df['Date'] + pd.Timedelta(days=1)

    # Group by the adjusted date
    grouped = df.groupby('AdjustedDate')

    # Calculate proportions of each state
    state_proportions = grouped['State'].value_counts(normalize=True).unstack(fill_value=0)

    # Calculate mean and std for each group
    mean_std = grouped['NormalizedPnL'].agg(['mean', 'std'])

    # Determine the state with the highest proportion for each adjusted date
    best_state = state_proportions.idxmax(axis=1)
    best_state.name = 'Best State'

    # Get the best states in order from best to worst
    best_states_sorted = state_proportions.apply(lambda row: row.sort_values(ascending=False).index.tolist(), axis=1)
    best_states_sorted.name = 'Best States Sorted'

    # Merge the proportions, mean, std, and best states sorted
    result = state_proportions.join(mean_std).join(best_state).join(best_states_sorted)

    # Calculate differences and sum for each state with the best mean
    df['AdjustedDate'] = df['AdjustedDate'].astype(str)
    result.index = result.index.astype(str)
    state_diffs = {}

    for adjusted_date in result.index:
        date_mean = result.loc[adjusted_date, 'mean']
        date_indices = df[df['AdjustedDate'] == adjusted_date].index
        differences = df.loc[date_indices, 'NormalizedPnL'] - date_mean
        for state in result.loc[adjusted_date, 'Best States Sorted']:
            state_diffs[state] = state_diffs.get(state, 0) + differences.sum()

    return result, state_diffs

# Example usage
csv_path = 'your_data.csv'
result_df, state_diffs = process_and_calculate_diff(csv_path)
print(result_df)
print(state_diffs)

  # Calculate mean and std for the best state within each group
    mean_std_best_state = grouped.apply(lambda x: x[x['State'] == best_state.loc[x.name]]['NormalizedPnL'].agg(['mean', 'std']))
import pandas as pd

def process_and_calculate_cumulative_mean(csv_path):
    # Load the data
    df = pd.read_csv(csv_path, parse_dates=['Timestamp'], index_col='Timestamp')

    # Add a date and day of the week column
    df['Date'] = df.index.date
    df['DayOfWeek'] = df.index.dayofweek

    # Adjust the date for Sunday to merge with Monday
    df['AdjustedDate'] = df['Date']
    df.loc[df['DayOfWeek'] == 6, 'AdjustedDate'] = df['Date'] + pd.Timedelta(days=1)

    # Group by the adjusted date
    grouped = df.groupby('AdjustedDate')

    # Calculate proportions of each state
    state_proportions = grouped['State'].value_counts(normalize=True).unstack(fill_value=0)

    # Determine the state with the highest proportion for each adjusted date
    best_state = state_proportions.idxmax(axis=1)
    best_state.name = 'Best State'

    # Initialize a DataFrame to store the results
    result = pd.DataFrame(index=state_proportions.index)
    result['Best State'] = best_state

    # Calculate cumulative mean for the best state up to and including the current date
    cumulative_means = []
    for date in result.index:
        filtered_df = df[df['AdjustedDate'] <= date]
        best_state_current = result.loc[date, 'Best State']
        cumulative_mean = filtered_df[filtered_df['State'] == best_state_current]['NormalizedPnL'].mean()
        cumulative_means.append(cumulative_mean)
    
    result['Cumulative Mean'] = cumulative_means

    # Calculate standard deviation for the best state within each group
    std_best_state = grouped.apply(lambda x: x[x['State'] == best_state.loc[x.name]]['NormalizedPnL'].std())
    result['Standard Deviation'] = std_best_state

    # Calculate differences and sum for each state with the best mean
    df['AdjustedDate'] = df['AdjustedDate'].astype(str)
    result.index = result.index.astype(str)
    state_diffs = {}

    for adjusted_date in result.index:
        date_mean = result.loc[adjusted_date, 'Cumulative Mean']
        date_indices = df[df['AdjustedDate'] == adjusted_date].index
        differences = df.loc[date_indices, 'NormalizedPnL'] - date_mean
        state = result.loc[adjusted_date, 'Best State']
        state_diffs[state] = state_diffs.get(state, 0) + differences.sum()

    return result, state_diffs

# Example usage
csv_path = 'your_data.csv'
result_df, state_diffs = process_and_calculate_cumulative_mean(csv_path)
print(result_df)
print(state_diffs)


import pandas as pd

# Sample data for the first table
data1 = """datetime,bids,asks
2024-07-25 12:34:56,1,2,5,7,3,4,6,8
2024-07-25 12:35:56,2,3,6,8,1,4,7,9"""

# Sample data for the second table
data2 = """datetime,mode
2024-07-25 12:34:00,mode1
2024-07-25 12:35:00,mode2
2024-07-25 12:36:00,mode3"""

# Read the data into DataFrames
df1 = pd.read_csv(pd.compat.StringIO(data1))
df2 = pd.read_csv(pd.compat.StringIO(data2))

# Function to convert the string "1,2,5,7" to a list [1,2,5,7]
def str_to_list(s):
    return [int(item) for item in s.split(',')]

# Apply the conversion to the 'bids' and 'asks' columns in the first DataFrame
df1['bids'] = df1['bids'].apply(str_to_list)
df1['asks'] = df1['asks'].apply(str_to_list)

# Convert the 'datetime' columns to pandas datetime
df1['datetime'] = pd.to_datetime(df1['datetime'])
df2['datetime'] = pd.to_datetime(df2['datetime'])

# Sort both DataFrames by 'datetime'
df1 = df1.sort_values('datetime')
df2 = df2.sort_values('datetime')

# Perform an asof merge
merged_df = pd.merge_asof(df1, df2, on='datetime', direction='backward')

# Display the merged DataFrame
print(merged_df)


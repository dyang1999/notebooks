{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l60PmOqHqNib"
   },
   "source": [
    "# 1. Load Packages Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJpZnsXcr83T",
    "outputId": "019fdad5-585a-41d2-a1e3-86e55766062e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (0.23.2)\n",
      "Requirement already satisfied: future in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (0.18.2)\n",
      "Requirement already satisfied: requests in c:\\users\\zywon\\appdata\\roaming\\python\\python37\\site-packages (from keras-tuner) (2.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (1.20.0)\n",
      "Requirement already satisfied: terminaltables in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (0.4.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (4.62.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (1.5.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from keras-tuner) (0.8.9)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from scikit-learn->keras-tuner) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from scikit-learn->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from requests->keras-tuner) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zywon\\appdata\\roaming\\python\\python37\\site-packages (from requests->keras-tuner) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: msoffcrypto-tool in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: cryptography>=2.3 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from msoffcrypto-tool) (2.9.2)\n",
      "Requirement already satisfied: olefile>=0.45 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from msoffcrypto-tool) (0.46)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from cryptography>=2.3->msoffcrypto-tool) (1.14.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from cryptography>=2.3->msoffcrypto-tool) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->msoffcrypto-tool) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner\n",
    "!pip install msoffcrypto-tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTPtLUFTqNis",
    "outputId": "9a42de20-551b-4738-c353-87acc66122fd"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from kerastuner.applications import HyperResNet\n",
    "from kerastuner.tuners import Hyperband\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import msoffcrypto\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmg2UIVGqNiz"
   },
   "source": [
    "# 3. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19DbR-pcqNi0",
    "outputId": "6de7ad31-c5ab-4019-bf80-9d2426950175"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_list = list()\n",
    "    \n",
    "raw = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zdGU_sTqNjD"
   },
   "source": [
    "# 6. Functions for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnnW9PNDqNjD"
   },
   "source": [
    "## Backfill Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "XF1J5vKiqNjD"
   },
   "outputs": [],
   "source": [
    "def missing_data(df, targeted_length):\n",
    "    \n",
    "    curr_length = len(df)\n",
    "\n",
    "    if curr_length != targeted_length:\n",
    "        df_2016_03 = df[(df['Year'] == 2016) & (df['Month'] == 4)]\n",
    "\n",
    "        if len(df_2016_03) != 0 and (curr_length + 1) == targeted_length:\n",
    "            df_2016_03 = df_2016_03.reset_index()\n",
    "            df_2016_03.loc[0, 'Month'] = 3\n",
    "\n",
    "            frames = [df, df_2016_03]\n",
    "            df_final = pd.concat(frames)\n",
    "            df_final = df_final.reset_index(drop=True)\n",
    "            df_final = df_final.sort_values(['Year', 'Month'], ascending=[True, True])\n",
    "\n",
    "        else:\n",
    "            mux = pd.MultiIndex.from_product([df['placeholder'].unique(),\n",
    "                                      range(2015,2021),\n",
    "                                      range(1, 13)], names=['placeholder','Year','Month'])\n",
    "\n",
    "            df_final = df.set_index(['placeholder','Year','Month']).reindex(mux, fill_value=0).reset_index()\n",
    "    \n",
    "    result = df_final\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LedZezeqNjE"
   },
   "source": [
    "## Stationarity Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Qb7SB_VUqNjE"
   },
   "outputs": [],
   "source": [
    "def adt_check(ts):\n",
    "    # Store Stationary/Non Stationary for 3 cases, Normal, Detrended and Lagged(Differenced)\n",
    "    patterns = []\n",
    "    #data = [ts.data, ts.z_data.dropna(), ts.zp_data.dropna()]\n",
    "    data = [ts.data]\n",
    "    for i in data:\n",
    "        dftest = adfuller(i, autolag='AIC')\n",
    "        if dftest[4]['5%'] < dftest[0]:\n",
    "            patterns.append([0,1])\n",
    "        else:\n",
    "            patterns.append([1,0])\n",
    "    return patterns\n",
    "    \n",
    "def kpss_check(ts):\n",
    "    # Store Stationary/Non Stationary for 3 cases, Normal, Detrended and Lagged(Differenced)\n",
    "    patterns = []\n",
    "    data = [ts.data]\n",
    "    for i in data:\n",
    "        dftest = kpss(ts.data, regression='c')\n",
    "        if dftest[3]['5%'] < dftest[0]:\n",
    "            patterns.append([0,1])\n",
    "        else:\n",
    "            patterns.append([1,0])\n",
    "    return patterns\n",
    "    \n",
    "def stationary_check(ts):\n",
    "    ts['z_data'] = np.log(ts['data'])\n",
    "    ts['zp_data'] = ts['z_data'] - ts['z_data'].shift(1)\n",
    "    adt_status = adt_check(ts)\n",
    "    kpss_status = kpss_check(ts) \n",
    "    patterns = [x + y for x, y in zip(adt_status, kpss_status)]\n",
    "    # If both stationary\n",
    "    if patterns[0] == [1, 0, 1, 0]:\n",
    "        return 'stationary'\n",
    "    elif patterns[0] == [0, 1, 0, 1]:\n",
    "        return 'non_stationary'\n",
    "    else:\n",
    "        # ADF - Stationary and KPSS - Non Stationary\n",
    "        if patterns[0][0] == 1 and patterns[0][3] == 1:\n",
    "            return 'trend'\n",
    "        elif patterns[0][1] == 1 and patterns[0][2] == 1:\n",
    "            return 'difference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "gyYKuUueO3wW"
   },
   "outputs": [],
   "source": [
    "def log_transform(data, inverse=False):\n",
    "    if inverse: \n",
    "        return np.exp(data) + 1\n",
    "    return np.log(data + 1)\n",
    "\n",
    "def trend_transform(data, mean=None, std=None, inverse=False):\n",
    "    if inverse:\n",
    "        std = std[-(len(data)):]\n",
    "        mean = mean[-(len(data)):]\n",
    "        step1 = [x * y for (x, y) in zip(data, std)]\n",
    "        step2 = [x + y for (x, y) in zip(step1, mean)]\n",
    "        return step2\n",
    "    return ((data - data.rolling(window=12).mean()) / data.rolling(window=12).std()), data.rolling(window=12).mean().values, data.rolling(window=12).std().values\n",
    "\n",
    "def difference_transform(data, inverse=False):\n",
    "    if inverse:\n",
    "        shifted = data[0] + data[:-1]\n",
    "        return  [x + y for (x, y) in zip(data, shifted)]\n",
    "    return data - data.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUvvFPIMqNjF"
   },
   "source": [
    "## LSTM Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "aRt15Jq4qNjG"
   },
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "H4R5ezkVqNjG"
   },
   "outputs": [],
   "source": [
    "def loss_plots(model, train_X, train_y, test_X, test_y):\n",
    "    history = model.fit(train_X, train_y, \n",
    "                        epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.title('placeholder')\n",
    "    title = 'placeholder' + '.png'\n",
    "    pyplot.savefig(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "t4xtAczbRVjq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pmdarima in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (1.8.3)\n",
      "Requirement already satisfied: statsmodels!=0.12.0,>=0.11 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (0.12.2)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (49.6.0.post20200814)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\zywon\\appdata\\roaming\\python\\python37\\site-packages (from pmdarima) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (1.20.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (1.5.0)\n",
      "Requirement already satisfied: Cython!=0.29.18,>=0.29 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (0.29.24)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (0.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\zywon\\appdata\\roaming\\python\\python37\\site-packages (from pmdarima) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pmdarima) (0.23.2)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from statsmodels!=0.12.0,>=0.11->pmdarima) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from pandas>=0.19->pmdarima) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from scikit-learn>=0.22->pmdarima) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\zywon\\anaconda3\\envs\\py37\\lib\\site-packages (from patsy>=0.5->statsmodels!=0.12.0,>=0.11->pmdarima) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt\n",
    "import xgboost\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "!pip install pmdarima\n",
    "#!pip install pandas==0.24\n",
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLR1HYrOqNjH"
   },
   "source": [
    "# 7. Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "vwcyu9ZFega5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Holt ES\n",
    "def holt_es(train, test):\n",
    "    model = Holt(np.asarray(train))\n",
    "    fit1 = model.fit(optimized=True)\n",
    "    pred1 = fit1.forecast(len(test))\n",
    "    return pred1\n",
    "\n",
    "# baseline model - persistence model\n",
    "def model_persistence(x):\n",
    "    return x\n",
    "\n",
    "def baseline_model(test):\n",
    "    predictions = list()\n",
    "    for x in test:\n",
    "        yhat = model_persistence(x)\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n",
    "\n",
    "def arima_model(train, test):\n",
    "    try:\n",
    "        model = pm.auto_arima(train, start_p=0, start_q=1,\n",
    "                            test='adf',       # use adftest to find optimal 'd'\n",
    "                            max_p=6, max_q=6, # maximum p and q\n",
    "                            m=1,              # frequency of series\n",
    "                            d=None,           # let model determine 'd'\n",
    "                            seasonal=False,   # No Seasonality\n",
    "      #                       start_P=0, \n",
    "                            D=0, \n",
    "      #                       trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "    except:\n",
    "        model = pm.auto_arima(train, start_p=0, start_q=1,\n",
    "                            max_p=6, max_q=6, # maximum p and q\n",
    "                            m=1,              # frequency of series\n",
    "                            d=None,           # let model determine 'd'\n",
    "                            seasonal=False,   # No Seasonality\n",
    "      #                       start_P=0, \n",
    "                            D=0, \n",
    "      #                       trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "    model_fit = model\n",
    "    # make predictions\n",
    "    predictions = model_fit.predict(n_periods = len(test))\n",
    "    return predictions \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "a1zK1VUcqNjH",
    "outputId": "f489e03d-92b3-4ce1-b53c-40e87f5dc7d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "for k in ['hi']:\n",
    "    output_dict[k] = dict()\n",
    "    for i in range(len('placeholder')):\n",
    "        try:\n",
    "            df_final = df_final.astype(int)\n",
    "\n",
    "            # Do Stationary Checks for Cases\n",
    "            ts = pd.DataFrame(df_final[k])\n",
    "            ts.columns = ['data']\n",
    "            ts = ts.sort_index()\n",
    "            stnry = stationary_check(ts)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if stnry == 'non_stationary':\n",
    "            df_final[k] = log_transform(df_final[k])\n",
    "        elif stnry == 'trend':\n",
    "            df_final[k], mean_k, std_k = trend_transform(df_final[k])\n",
    "        elif stnry == 'difference':\n",
    "            df_final[k] = difference_transform(df_final[k])\n",
    "        df_final = df_final.dropna() \n",
    "\n",
    "        # UNIVARIATE\n",
    "        values = pd.DataFrame(df_final[k])\n",
    "        # Hold out\n",
    "        num_holdout = 16\n",
    "        # Running baseline\n",
    "        #create lag\n",
    "        lagged_df = pd.concat([values.shift(1), values], axis=1)\n",
    "        lagged_df.columns = ['t-1', 't+1']\n",
    "        # load dataset\n",
    "        values = lagged_df.values\n",
    "        # ensure all data is float\n",
    "        values = values.astype('float32')\n",
    "        # scale data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 3))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        values = scaled\n",
    "\n",
    "        #split into train and test set\n",
    "        num_holdout = 16\n",
    "        train = values[1:len(values)-num_holdout, :]\n",
    "        test = values[len(values)-num_holdout:, :]\n",
    "        train_X, train_y = train[:,0], train[:,1]\n",
    "        test_X, test_y = test[:,0], test[:,1]\n",
    "        predictions = baseline_model(test_X)\n",
    "\n",
    "        if stnry == 'non_stationary':\n",
    "            test_y_t, predictions = log_transform(test_y, inverse=True), log_transform(predictions, inverse=True)\n",
    "        elif stnry == 'trend':\n",
    "            test_y_t, predictions = trend_transform(test_y, mean=mean_k, std=std_k, inverse=True), trend_transform(predictions, mean=mean_k, std=std_k, inverse=True)\n",
    "        elif stnry == 'difference':\n",
    "            test_y_t, predictions = difference_transform(test_y, inverse=True), difference_transform(predictions, inverse=True)\n",
    "        else:\n",
    "            test_y_t = test_y\n",
    "        df_final = df_final.dropna() \n",
    "        rmse_test = sqrt(mean_squared_error(test_y_t, predictions))\n",
    "        mae_test = mean_absolute_error(test_y_t, predictions)\n",
    "        names += [f'{k}_baseline_rmse', f'{k}_baseline_mae']\n",
    "        tests += [rmse_test, mae_test]\n",
    "\n",
    "        # Ridge Lasso\n",
    "        df_final = df_final.sort_index()\n",
    "        df_lagged = df_final[[k]]\n",
    "\n",
    "        for i in range(1,7):\n",
    "            df_lagged[\"lag_{}\".format(i)] = df_lagged[k].shift(i)\n",
    "        y = df_lagged.dropna()[k]\n",
    "        X = df_lagged.dropna().drop([k], axis=1)\n",
    "        train_y, test_y = y[:len(values)-num_holdout], y[len(values)-num_holdout:]\n",
    "        train_X, test_X = X[:len(values)-num_holdout], X[len(values)-num_holdout:]\n",
    "        Ridge = RidgeCV(alphas=np.arange(0.1,10,0.1), cv=10)\n",
    "        Ridge.fit(train_X, train_y)\n",
    "        predictions = Ridge.predict(test_X)\n",
    "        if stnry == 'non_stationary':\n",
    "            test_y_t, predictions = log_transform(test_y, inverse=True), log_transform(predictions, inverse=True)\n",
    "        elif stnry == 'trend':\n",
    "            test_y_t, predictions = trend_transform(test_y, mean=mean_k, std=std_k, inverse=True), trend_transform(predictions, mean=mean_k, std=std_k, inverse=True)\n",
    "        elif stnry == 'difference':\n",
    "            test_y_t, predictions = difference_transform(test_y, inverse=True), difference_transform(predictions, inverse=True)\n",
    "        else:\n",
    "            test_y_t = test_y\n",
    "        rmse_test = sqrt(mean_squared_error(test_y_t, predictions))\n",
    "        mae_test = mean_absolute_error(test_y_t, predictions)\n",
    "        \n",
    "        rmse_test = sqrt(mean_squared_error(test_y_t, predictions))\n",
    "        mae_test = mean_absolute_error(test_y_t, predictions)\n",
    "        names += [f'{k}_ridge_rmse', f'{k}_ridge_mae']\n",
    "        tests += [rmse_test, mae_test]\n",
    "        \n",
    "        # ARIMA \n",
    "        values = pd.DataFrame(df_final[k]).values\n",
    "        train = values[:len(values)-num_holdout,0]\n",
    "        test = values[len(values)-num_holdout:, 0]\n",
    "        predictions = arima_model(train, test)\n",
    "        # evaluate model\n",
    "        if stnry == 'non_stationary':\n",
    "            test_y_t, predictions = log_transform(test, inverse=True), log_transform(predictions, inverse=True)\n",
    "        elif stnry == 'trend':\n",
    "            test_y_t, predictions = trend_transform(test, mean=mean_k, std=std_k, inverse=True), trend_transform(predictions, mean=mean_k, std=std_k, inverse=True)\n",
    "        elif stnry == 'difference':\n",
    "            test_y_t, predictions = difference_transform(test, inverse=True), difference_transform(predictions, inverse=True)\n",
    "        else:\n",
    "            test_y_t = test\n",
    "        rmse_test = sqrt(mean_squared_error(test_y_t, predictions))\n",
    "        mae_test = mean_absolute_error(test_y_t, predictions)\n",
    "\n",
    "        rmse_test = sqrt(mean_squared_error(test_y_t, predictions))\n",
    "        mae_test = mean_absolute_error(test_y_t, predictions)\n",
    "        names += [f'{k}_arima_rmse', f'{k}_arima_mae']\n",
    "        tests += [rmse_test, mae_test]\n",
    "\n",
    "        output = pd.DataFrame([tests], columns=names)\n",
    "        output_dict[k]['placeholder'] = output\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Combined v1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

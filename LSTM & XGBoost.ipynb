{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l60PmOqHqNib"
   },
   "source": [
    "# 1. Load Packages Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xTPtLUFTqNis"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-59739833d370>:21: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.applications import HyperResNet\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from kerastuner.applications import HyperResNet\n",
    "from kerastuner.tuners import Hyperband\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import msoffcrypto\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmg2UIVGqNiz"
   },
   "source": [
    "# 3. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19DbR-pcqNi0",
    "outputId": "6250df9b-b9dd-4b34-eef6-7b54c7f8ea5d"
   },
   "outputs": [],
   "source": [
    "# Change to own directory\n",
    "# filenames just my files\n",
    "# just my dataset ignore\n",
    "\n",
    "df_list = list()\n",
    "    \n",
    "raw = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GMwYQisoteVq"
   },
   "outputs": [],
   "source": [
    "data = raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvgDfS7atgZ3",
    "outputId": "3bb81f8a-97fe-4824-baa6-c85f40cd5391"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "XKJPWW1UqNi3",
    "outputId": "54cd050c-1b54-4bd2-f81b-e53f5e4f6433"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zdGU_sTqNjD"
   },
   "source": [
    "# 6. Functions for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LedZezeqNjE"
   },
   "source": [
    "## Stationarity Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Qb7SB_VUqNjE"
   },
   "outputs": [],
   "source": [
    "def adt_check(ts):\n",
    "    # Store Stationary/Non Stationary for 3 cases, Normal, Detrended and Lagged(Differenced)\n",
    "    patterns = []\n",
    "    #data = [ts.data, ts.z_data.dropna(), ts.zp_data.dropna()]\n",
    "    data = [ts.data]\n",
    "    for i in data:\n",
    "        dftest = adfuller(i, autolag='AIC')\n",
    "        if dftest[4]['5%'] < dftest[0]:\n",
    "            patterns.append([0,1])\n",
    "        else:\n",
    "            patterns.append([1,0])\n",
    "    return patterns\n",
    "    \n",
    "def kpss_check(ts):\n",
    "    # Store Stationary/Non Stationary for 3 cases, Normal, Detrended and Lagged(Differenced)\n",
    "    patterns = []\n",
    "    data = [ts.data]\n",
    "    for i in data:\n",
    "        dftest = kpss(ts.data, regression='c')\n",
    "        if dftest[3]['5%'] < dftest[0]:\n",
    "            patterns.append([0,1])\n",
    "        else:\n",
    "            patterns.append([1,0])\n",
    "    return patterns\n",
    "    \n",
    "def stationary_check(ts):\n",
    "    ts['z_data'] = np.log(ts['data'])\n",
    "    ts['zp_data'] = ts['z_data'] - ts['z_data'].shift(1)\n",
    "    adt_status = adt_check(ts)\n",
    "    kpss_status = kpss_check(ts) \n",
    "    patterns = [x + y for x, y in zip(adt_status, kpss_status)]\n",
    "    # If both stationary\n",
    "    if patterns[0] == [1, 0, 1, 0]:\n",
    "        return 'stationary'\n",
    "    elif patterns[0] == [0, 1, 0, 1]:\n",
    "        return 'non_stationary'\n",
    "    else:\n",
    "        # ADF - Stationary and KPSS - Non Stationary\n",
    "        if patterns[0][0] == 1 and patterns[0][3] == 1:\n",
    "          return 'trend'\n",
    "        elif patterns[0][1] == 1 and patterns[0][2] == 1:\n",
    "          return 'difference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gyYKuUueO3wW"
   },
   "outputs": [],
   "source": [
    "def log_transform(data, inverse=False):\n",
    "  if inverse: \n",
    "    return np.exp(data) + 1\n",
    "  return np.log(data + 1)\n",
    "\n",
    "def trend_transform(data, inverse=False):\n",
    "  if inverse:\n",
    "    return (pd.DataFrame(data) * pd.DataFrame(data).rolling(window=12).std()) + pd.DataFrame(data).rolling(window=12).mean()\n",
    "  return (data - data.Cases.rolling(window=12).mean()) / data.Cases.rolling(window=12).std()\n",
    "\n",
    "def difference_transform(data, inverse=False):\n",
    "  if inverse:\n",
    "    shifted = data[0] + data[:-1]\n",
    "    return  [x + y for (x, y) in zip(data, shifted)]\n",
    "  return data - data.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUvvFPIMqNjF"
   },
   "source": [
    "## LSTM Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aRt15Jq4qNjG"
   },
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential(LSTM(hp.Int('input_unit', min_value=10, max_value=60, step=10),\n",
    "                                  return_sequences=True,\n",
    "                                  input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    for i in range(hp.Int('n_layers', 1,4)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units', min_value=10, max_value=60, step=10),\n",
    "                      return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hp.Int('input_unit', min_value=10, max_value=60, step=10),\n",
    "                                  input_shape=(train_X.shape[1], train_X.shape[2]))))\n",
    "    \n",
    "    model.add(Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics = ['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H4R5ezkVqNjG"
   },
   "outputs": [],
   "source": [
    "def loss_plots(model, train_X, train_y, test_X, test_y):\n",
    "    history = model.fit(train_X, train_y, \n",
    "                        epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    title = 'placeholder' + '.png'\n",
    "    pyplot.savefig(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLR1HYrOqNjH"
   },
   "source": [
    "# 7. Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vwcyu9ZFega5"
   },
   "outputs": [],
   "source": [
    "def lstm_model(train_X, test_X, train_y, test_y):\n",
    "    tuner = kt.RandomSearch(build_model,\n",
    "                                objective='val_loss',\n",
    "                                max_trials=10,\n",
    "                                overwrite = True)\n",
    "\n",
    "    tuner.search(train_X,train_y, validation_data=(test_X, test_y),\n",
    "                      epochs=50, batch_size=72)\n",
    "\n",
    "    fs_model = tuner.get_best_models(num_models=1)[0]\n",
    "    history = fs_model.fit(train_X, train_y, \n",
    "                                  epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "\n",
    "    yhat = fs_model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "\n",
    "\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "\n",
    "    return inv_y, inv_yhat\n",
    "\n",
    "\n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train, testX):\n",
    "    # transform list into array\n",
    "    train = np.asarray(train)\n",
    "    # split into input and output columns\n",
    "    trainX, trainy = train[:, :-1], train[:, -1]\n",
    "    # fit model\n",
    "    model = xgboost.XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "    trainX = np.ascontiguousarray(trainX)\n",
    "    trainy = np.ascontiguousarray(trainy)\n",
    "    model.fit(trainX, trainy)\n",
    "    # make a one-step prediction\n",
    "    testX = np.array([testX])\n",
    "    testX = np.ascontiguousarray(testX)\n",
    "    yhat = model.predict(testX)\n",
    "    return yhat[0]\n",
    "\n",
    "# walk-forward validation for multivariate data\n",
    "def walk_forward_validation(train ,test):\n",
    "    predictions = list()\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model on history and make a prediction\n",
    "        yhat = xgboost_forecast(history, test_X[i,:])\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i,:])\n",
    "        # summarize progress\n",
    "#         print('>expected=%.1f, predicted=%.1f' % (test_y[i], yhat))\n",
    "\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]))\n",
    "    yhat = np.array(predictions)\n",
    "    yhat = yhat.reshape((len(yhat),1))\n",
    "    inv_yhat = np.hstack((yhat, test_X[:, 1:]))\n",
    "\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    return inv_y, inv_yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dates(df):\n",
    "    y = np.array(df['Year']-1970, dtype='<M8[Y]')\n",
    "    m = np.array(df['Month']-1, dtype='<m8[M]')\n",
    "    d = np.array(0, dtype='<m8[D]')\n",
    "    dates = pd.Series(y+m+d)\n",
    "    \n",
    "    df.insert(len(df.columns), 'date', dates)\n",
    "    df = df.iloc[: , 4:]\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.index.name = 'date'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(df, targeted_length):\n",
    "    \n",
    "    curr_length = len(df)\n",
    "\n",
    "    if curr_length != targeted_length:\n",
    "        df_2016_03 = df[(df['Year'] == 2016) & (df['Month'] == 4)]\n",
    "\n",
    "        if len(df_2016_03) != 0 and (curr_length + 1) == targeted_length:\n",
    "            df_2016_03 = df_2016_03.reset_index()\n",
    "            df_2016_03.loc[0, 'Month'] = 3\n",
    "\n",
    "            frames = [df, df_2016_03]\n",
    "            df_final = pd.concat(frames)\n",
    "            df_final = df_final.reset_index(drop=True)\n",
    "            df_final = df_final.sort_values(['Year', 'Month'], ascending=[True, True])\n",
    "\n",
    "        else:\n",
    "            mux = pd.MultiIndex.from_product([df['placeholder'].unique(),\n",
    "                                      range(2015,2021),\n",
    "                                      range(1, 13)], names=['placeholder','Year','Month'])\n",
    "\n",
    "            df_final = df.set_index(['placeholder','Year','Month']).reindex(mux, fill_value=0).reset_index()\n",
    "    \n",
    "    result = df_final\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "variables = ['var1(t)','var2(t)', 'var3(t)', 'var4(t)', 'var5(t)',\n",
    "               'var6(t)', 'var7(t)', 'var8(t)', 'var9(t)', 'var10(t)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1zK1VUcqNjH",
    "outputId": "ddddab24-995f-4bc2-aaf4-b844dba842a1"
   },
   "outputs": [],
   "source": [
    "for i in range(len(placeholder)):\n",
    "    curr_cat = placeholder[i]\n",
    "    category_data = []\n",
    "    curr_drop = 'var' + str(i+1) + '(t)'\n",
    "    curr_var = list(variables)\n",
    "    if curr_drop in curr_var:\n",
    "        curr_var.remove(curr_drop)\n",
    "    print(curr_cat)\n",
    "        \n",
    "    for j in range(len(placeholder)):\n",
    "        curr_holder = placeholder[j]\n",
    "        curr_record = []\n",
    "        curr_record.append(curr_holder)\n",
    "        df_test = curr_df.loc[curr_df['placeholder'] == curr_holder]\n",
    "        df_inter = missing_data(df_test, 72)\n",
    "        df_final = add_dates(df_inter)\n",
    "        df_final = df_final.astype(int)\n",
    "\n",
    "        # Running LSTM\n",
    "        # load dataset\n",
    "        values = df_final.values\n",
    "\n",
    "        # ensure all data is float\n",
    "        values = values.astype('float32')\n",
    "\n",
    "        # scale data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 3))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, 1, 1)\n",
    "\n",
    "        reframed.drop(reframed[curr_var], axis=1, inplace=True)\n",
    "\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        n_train_hours = 56\n",
    "        train = values[:n_train_hours, :]\n",
    "        test = values[n_train_hours:, :]\n",
    "\n",
    "         # split into input and outputs\n",
    "        train_X, train_y = train[:, :-1], train[:, -1]\n",
    "        test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "        inv_y, inv_yhat = lstm_model(train_X, test_X, train_y, test_y)\n",
    "        lstm_rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        lstm_mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "        \n",
    "        curr_record.extend((lstm_rmse, lstm_mae))\n",
    "\n",
    "        # xgboost \n",
    "        inv_y, inv_yhat = walk_forward_validation(train, test)\n",
    "        xgboost_rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        xgboost_mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "        \n",
    "        curr_record.extend((xgboost_rmse, xgboost_mae))\n",
    "        \n",
    "        category_data.append(curr_record)\n",
    "    \n",
    "    output = pd.DataFrame(category_data, columns=['Placeholder', 'LSTM_RMSE', 'LSTM_MAE',\n",
    "                                                 'XGBoost_RMSE', 'XGBoost_MAE'])\n",
    "    curr_filename = '/Users/admin/Desktop/Y4S1/BT4103/' + curr_cat + '.csv'\n",
    "    output.to_csv(curr_filename, index=False)\n",
    "    print('---')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Combined v1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
